{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T18:06:58.607090Z",
     "iopub.status.busy": "2026-01-13T18:06:58.606742Z",
     "iopub.status.idle": "2026-01-13T18:06:58.651063Z",
     "shell.execute_reply": "2026-01-13T18:06:58.649970Z",
     "shell.execute_reply.started": "2026-01-13T18:06:58.607053Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> File converted to CoNLL-U+ and saved as demo/mi_texto.conllu\n"
     ]
    }
   ],
   "source": [
    "# Celda 3: Preparar texto\n",
    "texto = \"\"\"# Enūma Eliš\n",
    "e-nu-ma e-liš la na-bu-u2 šamû\n",
    "šap-liš am-ma-tum šu-ma la za-ak-rat\n",
    "ZU.AB-ma reš-tu-u2 za-ru-šu-un\n",
    "\"\"\"\n",
    "\n",
    "from txt2conllu import *\n",
    "\n",
    "with open('./demo/mi_texto.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(texto)\n",
    "\n",
    "!python txt2conllu.py --filename=demo/mi_texto.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T18:06:58.652893Z",
     "iopub.status.busy": "2026-01-13T18:06:58.652345Z",
     "iopub.status.idle": "2026-01-13T18:06:59.098291Z",
     "shell.execute_reply": "2026-01-13T18:06:59.096638Z",
     "shell.execute_reply.started": "2026-01-13T18:06:58.652852Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‹<>›‹<>›‹<>›‹<>›‹<>›‹<>›‹<>›‹<>›‹<>›‹<>›‹<>›‹<>›‹<>›‹<>›‹<>›‹<>›\n",
      "\n",
      "   BabyLemmatizer 2.2\n",
      "\n",
      "   A. Aleksi Sahala 2023-2024\n",
      "      + https://github.com/asahala\n",
      "\n",
      "   University of Helsinki\n",
      "      + Origins of Emesal Project\n",
      "      + Centre of Excellence for Ancient Near-Eastern Empires\n",
      "\n",
      "‹<>›‹<>›‹<>›‹<>›‹<>›‹<>›‹<>›‹<>›‹<>›‹<>›‹<>›‹<>›‹<>›‹<>›‹<>›‹<>›\n",
      "\n",
      "\n",
      "✓ Lemmatización con GPU completada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ochoa\\projects\\babel\\BabyLemmatizer\\babylemmatizer.py\", line 124, in <module>\n",
      "    lemmatizer = lemmatizer_pipeline.Lemmatizer(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ochoa\\projects\\babel\\BabyLemmatizer\\lemmatizer_pipeline.py\", line 84, in __init__\n",
      "    os.mkdir(step_path)\n",
      "FileNotFoundError: [WinError 3] The system cannot find the path specified: '/demo\\\\steps'\n",
      "'ls' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Celda 4: Lemmatizar CON GPU (sin --use-cpu)\n",
    "!python babylemmatizer.py \\\n",
    "    --lemmatize=lbtest2 \\\n",
    "    --filename=/demo/mi_texto.conllu\n",
    "    # Sin --use-cpu para usar GPU P100\n",
    "\n",
    "print(\"\\n✓ Lemmatización con GPU completada\")\n",
    "!ls -lh demo/mi_texto_*.conllu\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T18:07:10.274406Z",
     "iopub.status.busy": "2026-01-13T18:07:10.274045Z",
     "iopub.status.idle": "2026-01-13T18:07:10.631733Z",
     "shell.execute_reply": "2026-01-13T18:07:10.630914Z",
     "shell.execute_reply.started": "2026-01-13T18:07:10.274361Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           FORM LEMMA XPOS SCORE\n",
      "0       e-nu-ma     _    _     _\n",
      "1         e-liš     _    _     _\n",
      "2            la     _    _     _\n",
      "3      na-bu-u₂     _    _     _\n",
      "4          šamû     _    _     _\n",
      "5       šap-liš     _    _     _\n",
      "6     am-ma-tum     _    _     _\n",
      "7         šu-ma     _    _     _\n",
      "8            la     _    _     _\n",
      "9     za-ak-rat     _    _     _\n",
      "10     ZU.AB-ma     _    _     _\n",
      "11    reš-tu-u₂     _    _     _\n",
      "12  za-ru-šu-un     _    _     _\n"
     ]
    }
   ],
   "source": [
    "# Celda 5: Analizar resultados\n",
    "import pandas as pd\n",
    "\n",
    "with open('./demo/mi_texto.conllu', 'r', encoding='utf-8') as f:\n",
    "    lines = [l.strip() for l in f if l.strip() and not l.startswith('#')]\n",
    "\n",
    "data = []\n",
    "for line in lines:\n",
    "    fields = line.split('\\t')\n",
    "    if len(fields) >= 5:\n",
    "        data.append({\n",
    "            'FORM': fields[1],\n",
    "            'LEMMA': fields[2],\n",
    "            'XPOS': fields[4],\n",
    "            'SCORE': fields[15] if len(fields) > 15 else '_'\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Ejemplos de uso de BabyLemmatizer como librería\n",
    "    con soporte para archivos y objetos en memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from txt2conllu import txt_lines_to_conllu\n",
    "from lemmatizer_pipeline import Lemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uso clásico (archivo → archivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librería pura (lista → objeto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ejemplo_libreria_pura() -> None:\n",
    "    \"\"\"Uso como librería: todo en memoria, sin archivos\"\"\"\n",
    "    print(\"\\n=== EJEMPLO 2: Modo librería pura (lista → objeto) ===\")\n",
    "    \n",
    "    # Texto de entrada como lista\n",
    "    texto = [\n",
    "        \"e-nu-ma e-liš la na-bu-u₂ šamû\",\n",
    "        \"šap-liš am-ma-tum šu-ma la za-ak-rat\",\n",
    "        \"ZU.AB-ma reš-tu-u₂ za-ru-šu-un\"\n",
    "    ]\n",
    "    \n",
    "    # Paso 1: Convertir texto a objeto CoNLL-U\n",
    "    conllu_obj = txt_lines_to_conllu(lines=texto)\n",
    "    print(f\"✓ Creado objeto ConlluPlus con {conllu_obj.word_count} palabras\")\n",
    "    \n",
    "    # Paso 2: Lemmatizar en memoria (sin escribir archivos)\n",
    "    lemmatizer = Lemmatizer(input_file=conllu_obj)\n",
    "    resultado = lemmatizer.run_model(model_name='lbtest2', cpu=True)\n",
    "    \n",
    "    # Paso 3: Acceder a resultados en memoria\n",
    "    for id_, form, lemma, xpos in resultado.get_contents('id', 'form', 'lemma', 'xpos'):\n",
    "        print(f\"{id_}\\t{form}\\t{lemma}\\t{xpos}\")\n",
    "    \n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EJEMPLO 2: Modo librería pura (lista → objeto) ===\n",
      "✓ Creado objeto ConlluPlus con 13 palabras\n",
      "> Your model was trained with an old version of BabyLemmatizer.\n",
      "> Using Tokenizer setting 0. Rebuild model using --tokenizer.\n",
      "> Using tokenizer 0\n",
      "> Your model was trained with an old version of BabyLemmatizer.\n",
      "> Using default contexts\n",
      "> Tagger context = 2\n",
      "> Lemmatizer context = 1\n",
      "> Normalizing CoNLL-U\n",
      "> Updating field \"formctx\"\n",
      "> Fetching contexts for \"form\"\n",
      "> Generating input data for neural net (memory mode)\n",
      "> Input file size: 13 words in 3 segments.\n",
      "> Tagging with lbtest2\n",
      "> Updating field \"xpos\"\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\ochoa\\\\AppData\\\\Local\\\\Temp\\\\babylem_jqyigftq\\\\temp.tag_pred'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mejemplo_libreria_pura\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mejemplo_libreria_pura\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Paso 2: Lemmatizar en memoria (sin escribir archivos)\u001b[39;00m\n\u001b[32m     17\u001b[39m lemmatizer = Lemmatizer(input_file=conllu_obj)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m resultado = \u001b[43mlemmatizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlbtest2\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Paso 3: Acceder a resultados en memoria\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m id_, form, lemma, xpos \u001b[38;5;129;01min\u001b[39;00m resultado.get_contents(\u001b[33m'\u001b[39m\u001b[33mid\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mform\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mlemma\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mxpos\u001b[39m\u001b[33m'\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ochoa\\projects\\babel\\BabyLemmatizer\\lemmatizer_pipeline.py:193\u001b[39m, in \u001b[36mLemmatizer.run_model\u001b[39m\u001b[34m(self, model_name, cpu)\u001b[39m\n\u001b[32m    187\u001b[39m model_api.run_tagger(\u001b[38;5;28mself\u001b[39m.tagger_input,\n\u001b[32m    188\u001b[39m                      tagger_path,\n\u001b[32m    189\u001b[39m                      \u001b[38;5;28mself\u001b[39m.tagger_output,\n\u001b[32m    190\u001b[39m                      cpu)\n\u001b[32m    192\u001b[39m \u001b[38;5;66;03m# Merge tags to make lemmatizer input\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m \u001b[43mmodel_api\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmerge_tags\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtagger_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m                     \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msource_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m                     \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlemmatizer_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m                     \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mxpos\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m                     \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mxposctx\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[38;5;66;03m# Run lemmatizer\u001b[39;00m\n\u001b[32m    200\u001b[39m io(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mLemmatizing with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ochoa\\projects\\babel\\BabyLemmatizer\\model_api.py:69\u001b[39m, in \u001b[36mmerge_tags\u001b[39m\u001b[34m(neural_net_output, conllu_object, output_file, field, fieldctx)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m## TODO: tee tää suoraan tagger/lemmatisaattorikutsun\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m## yhteydessä, etenkin jos tästä tulee modulaarisempi\u001b[39;00m\n\u001b[32m     68\u001b[39m annotations = read_results(neural_net_output)\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m \u001b[43mconllu_object\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfield\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannotations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m contexts = {\u001b[33m'\u001b[39m\u001b[33mxpos\u001b[39m\u001b[33m'\u001b[39m: Context.lemmatizer_context,\n\u001b[32m     72\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mform\u001b[39m\u001b[33m'\u001b[39m: Context.tagger_context}\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fieldctx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ochoa\\projects\\babel\\BabyLemmatizer\\conlluplus.py:465\u001b[39m, in \u001b[36mConlluPlus.update_value\u001b[39m\u001b[34m(self, field, values)\u001b[39m\n\u001b[32m    457\u001b[39m     \u001b[38;5;66;03m#else:\u001b[39;00m\n\u001b[32m    458\u001b[39m     \u001b[38;5;66;03m#    vals = next(values)\u001b[39;00m\n\u001b[32m    459\u001b[39m     \u001b[38;5;66;03m#    for field, vals in zip(fields, zip(*vals)):\u001b[39;00m\n\u001b[32m    460\u001b[39m     \u001b[38;5;66;03m#        if isinstance(vals, (tuple, list)):\u001b[39;00m\n\u001b[32m    461\u001b[39m     \u001b[38;5;66;03m#            vals = '|'.join(vals)\u001b[39;00m\n\u001b[32m    462\u001b[39m     \u001b[38;5;66;03m#        sent[FIELDS[field]] = str(vals)\u001b[39;00m\n\u001b[32m    464\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m sent\n\u001b[32m--> \u001b[39m\u001b[32m465\u001b[39m \u001b[38;5;28mself\u001b[39m.data = \u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msents\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m             \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcomments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msents\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ochoa\\projects\\babel\\BabyLemmatizer\\conlluplus.py:465\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    457\u001b[39m     \u001b[38;5;66;03m#else:\u001b[39;00m\n\u001b[32m    458\u001b[39m     \u001b[38;5;66;03m#    vals = next(values)\u001b[39;00m\n\u001b[32m    459\u001b[39m     \u001b[38;5;66;03m#    for field, vals in zip(fields, zip(*vals)):\u001b[39;00m\n\u001b[32m    460\u001b[39m     \u001b[38;5;66;03m#        if isinstance(vals, (tuple, list)):\u001b[39;00m\n\u001b[32m    461\u001b[39m     \u001b[38;5;66;03m#            vals = '|'.join(vals)\u001b[39;00m\n\u001b[32m    462\u001b[39m     \u001b[38;5;66;03m#        sent[FIELDS[field]] = str(vals)\u001b[39;00m\n\u001b[32m    464\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m sent\n\u001b[32m--> \u001b[39m\u001b[32m465\u001b[39m \u001b[38;5;28mself\u001b[39m.data = [(comments, \u001b[43m[\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msents\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[32m    466\u001b[39m              \u001b[38;5;28;01mfor\u001b[39;00m comments, sents \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.data]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ochoa\\projects\\babel\\BabyLemmatizer\\conlluplus.py:465\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    457\u001b[39m     \u001b[38;5;66;03m#else:\u001b[39;00m\n\u001b[32m    458\u001b[39m     \u001b[38;5;66;03m#    vals = next(values)\u001b[39;00m\n\u001b[32m    459\u001b[39m     \u001b[38;5;66;03m#    for field, vals in zip(fields, zip(*vals)):\u001b[39;00m\n\u001b[32m    460\u001b[39m     \u001b[38;5;66;03m#        if isinstance(vals, (tuple, list)):\u001b[39;00m\n\u001b[32m    461\u001b[39m     \u001b[38;5;66;03m#            vals = '|'.join(vals)\u001b[39;00m\n\u001b[32m    462\u001b[39m     \u001b[38;5;66;03m#        sent[FIELDS[field]] = str(vals)\u001b[39;00m\n\u001b[32m    464\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m sent\n\u001b[32m--> \u001b[39m\u001b[32m465\u001b[39m \u001b[38;5;28mself\u001b[39m.data = [(comments, [\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sents])\n\u001b[32m    466\u001b[39m              \u001b[38;5;28;01mfor\u001b[39;00m comments, sents \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.data]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ochoa\\projects\\babel\\BabyLemmatizer\\conlluplus.py:447\u001b[39m, in \u001b[36mConlluPlus.update_value.<locals>.update\u001b[39m\u001b[34m(sent)\u001b[39m\n\u001b[32m    444\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m sent\n\u001b[32m    445\u001b[39m \u001b[38;5;66;03m#print(sent)\u001b[39;00m\n\u001b[32m    446\u001b[39m \u001b[38;5;66;03m#try:\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m447\u001b[39m vals = \u001b[38;5;28mnext\u001b[39m(values)\n\u001b[32m    448\u001b[39m \u001b[38;5;66;03m#print(vals, sent[:5])\u001b[39;00m\n\u001b[32m    449\u001b[39m \u001b[38;5;66;03m#except StopIteration:\u001b[39;00m\n\u001b[32m    450\u001b[39m \u001b[38;5;66;03m#    vals = ''\u001b[39;00m\n\u001b[32m    452\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(vals, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ochoa\\projects\\babel\\BabyLemmatizer\\model_api.py:43\u001b[39m, in \u001b[36mread_results\u001b[39m\u001b[34m(filename)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_results\u001b[39m(filename):\n\u001b[32m     42\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\" Read OpenNMT output file \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     44\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[32m     46\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m line.replace(\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m).rstrip()\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\ochoa\\\\AppData\\\\Local\\\\Temp\\\\babylem_jqyigftq\\\\temp.tag_pred'"
     ]
    }
   ],
   "source": [
    "ejemplo_libreria_pura()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Híbrido (lista → objeto → archivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ejemplo_hibrido():\n",
    "    \"\"\"Uso híbrido: procesar en memoria pero guardar archivo final\"\"\"\n",
    "    print(\"\\n=== EJEMPLO 3: Modo híbrido (lista → objeto → archivo) ===\")\n",
    "    \n",
    "    texto = [\n",
    "        \"e-nu-ma e-liš la na-bu-u₂ šamû\",\n",
    "        \"šap-liš am-ma-tum šu-ma la za-ak-rat\"\n",
    "    ]\n",
    "    \n",
    "    # Paso 1: Texto a objeto (opcionalmente también a archivo)\n",
    "    conllu_obj = txt_lines_to_conllu(texto, output=\"mi_texto.conllu\")\n",
    "    print(\"✓ Creado objeto Y guardado mi_texto.conllu\")\n",
    "    \n",
    "    # Paso 2: Lemmatizar y especificar archivo de salida\n",
    "    lemmatizer = Lemmatizer(conllu_obj, output_file=\"resultado.conllu\")\n",
    "    resultado = lemmatizer.run_model('lbtest2', cpu=True)\n",
    "    \n",
    "    print(\"✓ Resultado guardado en resultado.conllu\")\n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EJEMPLO 3: Modo híbrido (lista → objeto → archivo) ===\n",
      "> File converted to CoNLL-U+ and saved as mi_texto.conllu\n",
      "['e-nu-ma e-liš la na-bu-u₂ šamû', 'šap-liš am-ma-tum šu-ma la za-ak-rat']\n",
      "✓ Creado objeto Y guardado mi_texto.conllu\n",
      "> Your model was trained with an old version of BabyLemmatizer.\n",
      "> Using Tokenizer setting 0. Rebuild model using --tokenizer.\n",
      "> Using tokenizer 0\n",
      "> Your model was trained with an old version of BabyLemmatizer.\n",
      "> Using default contexts\n",
      "> Tagger context = 2\n",
      "> Lemmatizer context = 1\n",
      "> Normalizing CoNLL-U\n",
      "> Updating field \"formctx\"\n",
      "> Fetching contexts for \"form\"\n",
      "> Generating input data for neural net (memory mode)\n",
      "> Input file size: 10 words in 2 segments.\n",
      "> Tagging with lbtest2\n",
      "> Updating field \"xpos\"\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\ochoa\\\\AppData\\\\Local\\\\Temp\\\\babylem_q571ajcb\\\\temp.tag_pred'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mejemplo_hibrido\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mejemplo_hibrido\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Paso 2: Lemmatizar y especificar archivo de salida\u001b[39;00m\n\u001b[32m     16\u001b[39m lemmatizer = Lemmatizer(conllu_obj, output_file=\u001b[33m\"\u001b[39m\u001b[33mresultado.conllu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m resultado = \u001b[43mlemmatizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_model\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlbtest2\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✓ Resultado guardado en resultado.conllu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resultado\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ochoa\\projects\\babel\\BabyLemmatizer\\lemmatizer_pipeline.py:193\u001b[39m, in \u001b[36mLemmatizer.run_model\u001b[39m\u001b[34m(self, model_name, cpu)\u001b[39m\n\u001b[32m    187\u001b[39m model_api.run_tagger(\u001b[38;5;28mself\u001b[39m.tagger_input,\n\u001b[32m    188\u001b[39m                      tagger_path,\n\u001b[32m    189\u001b[39m                      \u001b[38;5;28mself\u001b[39m.tagger_output,\n\u001b[32m    190\u001b[39m                      cpu)\n\u001b[32m    192\u001b[39m \u001b[38;5;66;03m# Merge tags to make lemmatizer input\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m \u001b[43mmodel_api\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmerge_tags\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtagger_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m                     \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msource_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m                     \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlemmatizer_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m                     \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mxpos\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m                     \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mxposctx\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[38;5;66;03m# Run lemmatizer\u001b[39;00m\n\u001b[32m    200\u001b[39m io(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mLemmatizing with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ochoa\\projects\\babel\\BabyLemmatizer\\model_api.py:69\u001b[39m, in \u001b[36mmerge_tags\u001b[39m\u001b[34m(neural_net_output, conllu_object, output_file, field, fieldctx)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m## TODO: tee tää suoraan tagger/lemmatisaattorikutsun\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m## yhteydessä, etenkin jos tästä tulee modulaarisempi\u001b[39;00m\n\u001b[32m     68\u001b[39m annotations = read_results(neural_net_output)\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m \u001b[43mconllu_object\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfield\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannotations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m contexts = {\u001b[33m'\u001b[39m\u001b[33mxpos\u001b[39m\u001b[33m'\u001b[39m: Context.lemmatizer_context,\n\u001b[32m     72\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mform\u001b[39m\u001b[33m'\u001b[39m: Context.tagger_context}\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fieldctx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ochoa\\projects\\babel\\BabyLemmatizer\\conlluplus.py:465\u001b[39m, in \u001b[36mConlluPlus.update_value\u001b[39m\u001b[34m(self, field, values)\u001b[39m\n\u001b[32m    457\u001b[39m     \u001b[38;5;66;03m#else:\u001b[39;00m\n\u001b[32m    458\u001b[39m     \u001b[38;5;66;03m#    vals = next(values)\u001b[39;00m\n\u001b[32m    459\u001b[39m     \u001b[38;5;66;03m#    for field, vals in zip(fields, zip(*vals)):\u001b[39;00m\n\u001b[32m    460\u001b[39m     \u001b[38;5;66;03m#        if isinstance(vals, (tuple, list)):\u001b[39;00m\n\u001b[32m    461\u001b[39m     \u001b[38;5;66;03m#            vals = '|'.join(vals)\u001b[39;00m\n\u001b[32m    462\u001b[39m     \u001b[38;5;66;03m#        sent[FIELDS[field]] = str(vals)\u001b[39;00m\n\u001b[32m    464\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m sent\n\u001b[32m--> \u001b[39m\u001b[32m465\u001b[39m \u001b[38;5;28mself\u001b[39m.data = \u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msents\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m             \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcomments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msents\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ochoa\\projects\\babel\\BabyLemmatizer\\conlluplus.py:465\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    457\u001b[39m     \u001b[38;5;66;03m#else:\u001b[39;00m\n\u001b[32m    458\u001b[39m     \u001b[38;5;66;03m#    vals = next(values)\u001b[39;00m\n\u001b[32m    459\u001b[39m     \u001b[38;5;66;03m#    for field, vals in zip(fields, zip(*vals)):\u001b[39;00m\n\u001b[32m    460\u001b[39m     \u001b[38;5;66;03m#        if isinstance(vals, (tuple, list)):\u001b[39;00m\n\u001b[32m    461\u001b[39m     \u001b[38;5;66;03m#            vals = '|'.join(vals)\u001b[39;00m\n\u001b[32m    462\u001b[39m     \u001b[38;5;66;03m#        sent[FIELDS[field]] = str(vals)\u001b[39;00m\n\u001b[32m    464\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m sent\n\u001b[32m--> \u001b[39m\u001b[32m465\u001b[39m \u001b[38;5;28mself\u001b[39m.data = [(comments, \u001b[43m[\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msents\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[32m    466\u001b[39m              \u001b[38;5;28;01mfor\u001b[39;00m comments, sents \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.data]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ochoa\\projects\\babel\\BabyLemmatizer\\conlluplus.py:465\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    457\u001b[39m     \u001b[38;5;66;03m#else:\u001b[39;00m\n\u001b[32m    458\u001b[39m     \u001b[38;5;66;03m#    vals = next(values)\u001b[39;00m\n\u001b[32m    459\u001b[39m     \u001b[38;5;66;03m#    for field, vals in zip(fields, zip(*vals)):\u001b[39;00m\n\u001b[32m    460\u001b[39m     \u001b[38;5;66;03m#        if isinstance(vals, (tuple, list)):\u001b[39;00m\n\u001b[32m    461\u001b[39m     \u001b[38;5;66;03m#            vals = '|'.join(vals)\u001b[39;00m\n\u001b[32m    462\u001b[39m     \u001b[38;5;66;03m#        sent[FIELDS[field]] = str(vals)\u001b[39;00m\n\u001b[32m    464\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m sent\n\u001b[32m--> \u001b[39m\u001b[32m465\u001b[39m \u001b[38;5;28mself\u001b[39m.data = [(comments, [\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sents])\n\u001b[32m    466\u001b[39m              \u001b[38;5;28;01mfor\u001b[39;00m comments, sents \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.data]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ochoa\\projects\\babel\\BabyLemmatizer\\conlluplus.py:447\u001b[39m, in \u001b[36mConlluPlus.update_value.<locals>.update\u001b[39m\u001b[34m(sent)\u001b[39m\n\u001b[32m    444\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m sent\n\u001b[32m    445\u001b[39m \u001b[38;5;66;03m#print(sent)\u001b[39;00m\n\u001b[32m    446\u001b[39m \u001b[38;5;66;03m#try:\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m447\u001b[39m vals = \u001b[38;5;28mnext\u001b[39m(values)\n\u001b[32m    448\u001b[39m \u001b[38;5;66;03m#print(vals, sent[:5])\u001b[39;00m\n\u001b[32m    449\u001b[39m \u001b[38;5;66;03m#except StopIteration:\u001b[39;00m\n\u001b[32m    450\u001b[39m \u001b[38;5;66;03m#    vals = ''\u001b[39;00m\n\u001b[32m    452\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(vals, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ochoa\\projects\\babel\\BabyLemmatizer\\model_api.py:43\u001b[39m, in \u001b[36mread_results\u001b[39m\u001b[34m(filename)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_results\u001b[39m(filename):\n\u001b[32m     42\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\" Read OpenNMT output file \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     44\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[32m     46\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m line.replace(\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m).rstrip()\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\ochoa\\\\AppData\\\\Local\\\\Temp\\\\babylem_q571ajcb\\\\temp.tag_pred'"
     ]
    }
   ],
   "source": [
    "ejemplo_hibrido()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline completo en memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ejemplo_pipeline_completo():\n",
    "    \"\"\"Pipeline completo: texto → procesamiento → análisis\"\"\"\n",
    "    print(\"\\n=== EJEMPLO 4: Pipeline completo en memoria ===\")\n",
    "    \n",
    "    # Entrada\n",
    "    texto = [\n",
    "        \"e-nu-ma e-liš la na-bu-u₂ šamû\",\n",
    "        \"šap-liš am-ma-tum šu-ma la za-ak-rat\",\n",
    "        \"ZU.AB-ma reš-tu-u₂ za-ru-šu-un\"\n",
    "    ]\n",
    "    \n",
    "    # Paso 1: txt → conllu\n",
    "    conllu = txt_lines_to_conllu(texto)\n",
    "    \n",
    "    # Paso 2: lemmatizar\n",
    "    lem = Lemmatizer(conllu, ignore_numbers=True)\n",
    "    resultado = lem.run_model('lbtest2', cpu=True)\n",
    "    \n",
    "    # Paso 3: análisis en memoria\n",
    "    lemas_unicos = set()\n",
    "    pos_tags = {}\n",
    "    \n",
    "    for _, form, lemma, xpos in resultado.get_contents('id', 'form', 'lemma', 'xpos'):\n",
    "        if lemma != '_':\n",
    "            lemas_unicos.add(lemma)\n",
    "        if xpos != '_':\n",
    "            pos_tags[form] = xpos\n",
    "    \n",
    "    print(f\"\\n✓ Total de lemas únicos: {len(lemas_unicos)}\")\n",
    "    print(f\"✓ Formas con POS tag: {len(pos_tags)}\")\n",
    "    \n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EJEMPLO 4: Pipeline completo en memoria ===\n",
      "> Your model was trained with an old version of BabyLemmatizer.\n",
      "> Using Tokenizer setting 0. Rebuild model using --tokenizer.\n",
      "> Using tokenizer 0\n",
      "> Your model was trained with an old version of BabyLemmatizer.\n",
      "> Using default contexts\n",
      "> Tagger context = 2\n",
      "> Lemmatizer context = 1\n",
      "> Normalizing CoNLL-U\n",
      "> Updating field \"formctx\"\n",
      "> Fetching contexts for \"form\"\n",
      "> Generating input data for neural net (memory mode)\n",
      "> Input file size: 13 words in 3 segments.\n",
      "> Tagging with lbtest2\n",
      "> Updating field \"xpos\"\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\ochoa\\\\AppData\\\\Local\\\\Temp\\\\babylem_e24y105q\\\\temp.tag_pred'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mejemplo_pipeline_completo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mejemplo_pipeline_completo\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Paso 2: lemmatizar\u001b[39;00m\n\u001b[32m     16\u001b[39m lem = Lemmatizer(conllu, ignore_numbers=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m resultado = \u001b[43mlem\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_model\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlbtest2\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Paso 3: análisis en memoria\u001b[39;00m\n\u001b[32m     20\u001b[39m lemas_unicos = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ochoa\\projects\\babel\\BabyLemmatizer\\lemmatizer_pipeline.py:193\u001b[39m, in \u001b[36mLemmatizer.run_model\u001b[39m\u001b[34m(self, model_name, cpu)\u001b[39m\n\u001b[32m    187\u001b[39m model_api.run_tagger(\u001b[38;5;28mself\u001b[39m.tagger_input,\n\u001b[32m    188\u001b[39m                      tagger_path,\n\u001b[32m    189\u001b[39m                      \u001b[38;5;28mself\u001b[39m.tagger_output,\n\u001b[32m    190\u001b[39m                      cpu)\n\u001b[32m    192\u001b[39m \u001b[38;5;66;03m# Merge tags to make lemmatizer input\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m \u001b[43mmodel_api\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmerge_tags\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtagger_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m                     \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msource_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m                     \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlemmatizer_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m                     \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mxpos\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m                     \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mxposctx\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[38;5;66;03m# Run lemmatizer\u001b[39;00m\n\u001b[32m    200\u001b[39m io(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mLemmatizing with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ochoa\\projects\\babel\\BabyLemmatizer\\model_api.py:69\u001b[39m, in \u001b[36mmerge_tags\u001b[39m\u001b[34m(neural_net_output, conllu_object, output_file, field, fieldctx)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m## TODO: tee tää suoraan tagger/lemmatisaattorikutsun\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m## yhteydessä, etenkin jos tästä tulee modulaarisempi\u001b[39;00m\n\u001b[32m     68\u001b[39m annotations = read_results(neural_net_output)\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m \u001b[43mconllu_object\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfield\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannotations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m contexts = {\u001b[33m'\u001b[39m\u001b[33mxpos\u001b[39m\u001b[33m'\u001b[39m: Context.lemmatizer_context,\n\u001b[32m     72\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mform\u001b[39m\u001b[33m'\u001b[39m: Context.tagger_context}\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fieldctx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ochoa\\projects\\babel\\BabyLemmatizer\\conlluplus.py:465\u001b[39m, in \u001b[36mConlluPlus.update_value\u001b[39m\u001b[34m(self, field, values)\u001b[39m\n\u001b[32m    457\u001b[39m     \u001b[38;5;66;03m#else:\u001b[39;00m\n\u001b[32m    458\u001b[39m     \u001b[38;5;66;03m#    vals = next(values)\u001b[39;00m\n\u001b[32m    459\u001b[39m     \u001b[38;5;66;03m#    for field, vals in zip(fields, zip(*vals)):\u001b[39;00m\n\u001b[32m    460\u001b[39m     \u001b[38;5;66;03m#        if isinstance(vals, (tuple, list)):\u001b[39;00m\n\u001b[32m    461\u001b[39m     \u001b[38;5;66;03m#            vals = '|'.join(vals)\u001b[39;00m\n\u001b[32m    462\u001b[39m     \u001b[38;5;66;03m#        sent[FIELDS[field]] = str(vals)\u001b[39;00m\n\u001b[32m    464\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m sent\n\u001b[32m--> \u001b[39m\u001b[32m465\u001b[39m \u001b[38;5;28mself\u001b[39m.data = \u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msents\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m             \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcomments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msents\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ochoa\\projects\\babel\\BabyLemmatizer\\conlluplus.py:465\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    457\u001b[39m     \u001b[38;5;66;03m#else:\u001b[39;00m\n\u001b[32m    458\u001b[39m     \u001b[38;5;66;03m#    vals = next(values)\u001b[39;00m\n\u001b[32m    459\u001b[39m     \u001b[38;5;66;03m#    for field, vals in zip(fields, zip(*vals)):\u001b[39;00m\n\u001b[32m    460\u001b[39m     \u001b[38;5;66;03m#        if isinstance(vals, (tuple, list)):\u001b[39;00m\n\u001b[32m    461\u001b[39m     \u001b[38;5;66;03m#            vals = '|'.join(vals)\u001b[39;00m\n\u001b[32m    462\u001b[39m     \u001b[38;5;66;03m#        sent[FIELDS[field]] = str(vals)\u001b[39;00m\n\u001b[32m    464\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m sent\n\u001b[32m--> \u001b[39m\u001b[32m465\u001b[39m \u001b[38;5;28mself\u001b[39m.data = [(comments, \u001b[43m[\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msents\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[32m    466\u001b[39m              \u001b[38;5;28;01mfor\u001b[39;00m comments, sents \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.data]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ochoa\\projects\\babel\\BabyLemmatizer\\conlluplus.py:465\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    457\u001b[39m     \u001b[38;5;66;03m#else:\u001b[39;00m\n\u001b[32m    458\u001b[39m     \u001b[38;5;66;03m#    vals = next(values)\u001b[39;00m\n\u001b[32m    459\u001b[39m     \u001b[38;5;66;03m#    for field, vals in zip(fields, zip(*vals)):\u001b[39;00m\n\u001b[32m    460\u001b[39m     \u001b[38;5;66;03m#        if isinstance(vals, (tuple, list)):\u001b[39;00m\n\u001b[32m    461\u001b[39m     \u001b[38;5;66;03m#            vals = '|'.join(vals)\u001b[39;00m\n\u001b[32m    462\u001b[39m     \u001b[38;5;66;03m#        sent[FIELDS[field]] = str(vals)\u001b[39;00m\n\u001b[32m    464\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m sent\n\u001b[32m--> \u001b[39m\u001b[32m465\u001b[39m \u001b[38;5;28mself\u001b[39m.data = [(comments, [\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sents])\n\u001b[32m    466\u001b[39m              \u001b[38;5;28;01mfor\u001b[39;00m comments, sents \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.data]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ochoa\\projects\\babel\\BabyLemmatizer\\conlluplus.py:447\u001b[39m, in \u001b[36mConlluPlus.update_value.<locals>.update\u001b[39m\u001b[34m(sent)\u001b[39m\n\u001b[32m    444\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m sent\n\u001b[32m    445\u001b[39m \u001b[38;5;66;03m#print(sent)\u001b[39;00m\n\u001b[32m    446\u001b[39m \u001b[38;5;66;03m#try:\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m447\u001b[39m vals = \u001b[38;5;28mnext\u001b[39m(values)\n\u001b[32m    448\u001b[39m \u001b[38;5;66;03m#print(vals, sent[:5])\u001b[39;00m\n\u001b[32m    449\u001b[39m \u001b[38;5;66;03m#except StopIteration:\u001b[39;00m\n\u001b[32m    450\u001b[39m \u001b[38;5;66;03m#    vals = ''\u001b[39;00m\n\u001b[32m    452\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(vals, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ochoa\\projects\\babel\\BabyLemmatizer\\model_api.py:43\u001b[39m, in \u001b[36mread_results\u001b[39m\u001b[34m(filename)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_results\u001b[39m(filename):\n\u001b[32m     42\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\" Read OpenNMT output file \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     44\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[32m     46\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m line.replace(\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m).rstrip()\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\ochoa\\\\AppData\\\\Local\\\\Temp\\\\babylem_e24y105q\\\\temp.tag_pred'"
     ]
    }
   ],
   "source": [
    "ejemplo_pipeline_completo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ejemplo_batch():\n",
    "    \"\"\"Procesar múltiples textos en batch\"\"\"\n",
    "    print(\"\\n=== EJEMPLO 5: Batch processing ===\")\n",
    "    \n",
    "    textos = [\n",
    "        [\"e-nu-ma e-liš la na-bu-u₂ šamû\"],\n",
    "        [\"šap-liš am-ma-tum šu-ma\"],\n",
    "        [\"ZU.AB-ma reš-tu-u₂ za-ru-šu-un\"]\n",
    "    ]\n",
    "    \n",
    "    resultados = []\n",
    "    \n",
    "    for i, texto in enumerate(textos, 1):\n",
    "        print(f\"\\nProcesando texto {i}/{len(textos)}...\")\n",
    "        conllu = txt_lines_to_conllu(texto)\n",
    "        lem = Lemmatizer(conllu)\n",
    "        resultado = lem.run_model('lbtest2', cpu=True)\n",
    "        resultados.append(resultado)\n",
    "    \n",
    "    print(f\"\\n✓ Procesados {len(resultados)} textos\")\n",
    "    return resultados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EJEMPLO 5: Batch processing ===\n",
      "\n",
      "Procesando texto 1/3...\n",
      "> Your model was trained with an old version of BabyLemmatizer.\n",
      "> Using Tokenizer setting 0. Rebuild model using --tokenizer.\n",
      "> Using tokenizer 0\n",
      "> Your model was trained with an old version of BabyLemmatizer.\n",
      "> Using default contexts\n",
      "> Tagger context = 2\n",
      "> Lemmatizer context = 1\n",
      "> Normalizing CoNLL-U\n",
      "> Updating field \"formctx\"\n",
      "> Fetching contexts for \"form\"\n",
      "> Generating input data for neural net (memory mode)\n",
      "> Input file size: 5 words in 1 segments.\n",
      "> Tagging with lbtest2\n",
      "> Updating field \"xpos\"\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\ochoa\\\\AppData\\\\Local\\\\Temp\\\\babylem_tg_w0h1l\\\\temp.tag_pred'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mejemplo_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mejemplo_batch\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     15\u001b[39m     conllu = txt_lines_to_conllu(texto)\n\u001b[32m     16\u001b[39m     lem = Lemmatizer(conllu)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     resultado = \u001b[43mlem\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_model\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlbtest2\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     resultados.append(resultado)\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m✓ Procesados \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(resultados)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m textos\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ochoa\\projects\\babel\\BabyLemmatizer\\lemmatizer_pipeline.py:193\u001b[39m, in \u001b[36mLemmatizer.run_model\u001b[39m\u001b[34m(self, model_name, cpu)\u001b[39m\n\u001b[32m    187\u001b[39m model_api.run_tagger(\u001b[38;5;28mself\u001b[39m.tagger_input,\n\u001b[32m    188\u001b[39m                      tagger_path,\n\u001b[32m    189\u001b[39m                      \u001b[38;5;28mself\u001b[39m.tagger_output,\n\u001b[32m    190\u001b[39m                      cpu)\n\u001b[32m    192\u001b[39m \u001b[38;5;66;03m# Merge tags to make lemmatizer input\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m \u001b[43mmodel_api\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmerge_tags\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtagger_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m                     \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msource_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m                     \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlemmatizer_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m                     \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mxpos\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m                     \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mxposctx\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[38;5;66;03m# Run lemmatizer\u001b[39;00m\n\u001b[32m    200\u001b[39m io(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mLemmatizing with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ochoa\\projects\\babel\\BabyLemmatizer\\model_api.py:69\u001b[39m, in \u001b[36mmerge_tags\u001b[39m\u001b[34m(neural_net_output, conllu_object, output_file, field, fieldctx)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m## TODO: tee tää suoraan tagger/lemmatisaattorikutsun\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m## yhteydessä, etenkin jos tästä tulee modulaarisempi\u001b[39;00m\n\u001b[32m     68\u001b[39m annotations = read_results(neural_net_output)\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m \u001b[43mconllu_object\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfield\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannotations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m contexts = {\u001b[33m'\u001b[39m\u001b[33mxpos\u001b[39m\u001b[33m'\u001b[39m: Context.lemmatizer_context,\n\u001b[32m     72\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mform\u001b[39m\u001b[33m'\u001b[39m: Context.tagger_context}\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fieldctx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ochoa\\projects\\babel\\BabyLemmatizer\\conlluplus.py:465\u001b[39m, in \u001b[36mConlluPlus.update_value\u001b[39m\u001b[34m(self, field, values)\u001b[39m\n\u001b[32m    457\u001b[39m     \u001b[38;5;66;03m#else:\u001b[39;00m\n\u001b[32m    458\u001b[39m     \u001b[38;5;66;03m#    vals = next(values)\u001b[39;00m\n\u001b[32m    459\u001b[39m     \u001b[38;5;66;03m#    for field, vals in zip(fields, zip(*vals)):\u001b[39;00m\n\u001b[32m    460\u001b[39m     \u001b[38;5;66;03m#        if isinstance(vals, (tuple, list)):\u001b[39;00m\n\u001b[32m    461\u001b[39m     \u001b[38;5;66;03m#            vals = '|'.join(vals)\u001b[39;00m\n\u001b[32m    462\u001b[39m     \u001b[38;5;66;03m#        sent[FIELDS[field]] = str(vals)\u001b[39;00m\n\u001b[32m    464\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m sent\n\u001b[32m--> \u001b[39m\u001b[32m465\u001b[39m \u001b[38;5;28mself\u001b[39m.data = \u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msents\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m             \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcomments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msents\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ochoa\\projects\\babel\\BabyLemmatizer\\conlluplus.py:465\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    457\u001b[39m     \u001b[38;5;66;03m#else:\u001b[39;00m\n\u001b[32m    458\u001b[39m     \u001b[38;5;66;03m#    vals = next(values)\u001b[39;00m\n\u001b[32m    459\u001b[39m     \u001b[38;5;66;03m#    for field, vals in zip(fields, zip(*vals)):\u001b[39;00m\n\u001b[32m    460\u001b[39m     \u001b[38;5;66;03m#        if isinstance(vals, (tuple, list)):\u001b[39;00m\n\u001b[32m    461\u001b[39m     \u001b[38;5;66;03m#            vals = '|'.join(vals)\u001b[39;00m\n\u001b[32m    462\u001b[39m     \u001b[38;5;66;03m#        sent[FIELDS[field]] = str(vals)\u001b[39;00m\n\u001b[32m    464\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m sent\n\u001b[32m--> \u001b[39m\u001b[32m465\u001b[39m \u001b[38;5;28mself\u001b[39m.data = [(comments, \u001b[43m[\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msents\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[32m    466\u001b[39m              \u001b[38;5;28;01mfor\u001b[39;00m comments, sents \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.data]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ochoa\\projects\\babel\\BabyLemmatizer\\conlluplus.py:465\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    457\u001b[39m     \u001b[38;5;66;03m#else:\u001b[39;00m\n\u001b[32m    458\u001b[39m     \u001b[38;5;66;03m#    vals = next(values)\u001b[39;00m\n\u001b[32m    459\u001b[39m     \u001b[38;5;66;03m#    for field, vals in zip(fields, zip(*vals)):\u001b[39;00m\n\u001b[32m    460\u001b[39m     \u001b[38;5;66;03m#        if isinstance(vals, (tuple, list)):\u001b[39;00m\n\u001b[32m    461\u001b[39m     \u001b[38;5;66;03m#            vals = '|'.join(vals)\u001b[39;00m\n\u001b[32m    462\u001b[39m     \u001b[38;5;66;03m#        sent[FIELDS[field]] = str(vals)\u001b[39;00m\n\u001b[32m    464\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m sent\n\u001b[32m--> \u001b[39m\u001b[32m465\u001b[39m \u001b[38;5;28mself\u001b[39m.data = [(comments, [\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sents])\n\u001b[32m    466\u001b[39m              \u001b[38;5;28;01mfor\u001b[39;00m comments, sents \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.data]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ochoa\\projects\\babel\\BabyLemmatizer\\conlluplus.py:447\u001b[39m, in \u001b[36mConlluPlus.update_value.<locals>.update\u001b[39m\u001b[34m(sent)\u001b[39m\n\u001b[32m    444\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m sent\n\u001b[32m    445\u001b[39m \u001b[38;5;66;03m#print(sent)\u001b[39;00m\n\u001b[32m    446\u001b[39m \u001b[38;5;66;03m#try:\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m447\u001b[39m vals = \u001b[38;5;28mnext\u001b[39m(values)\n\u001b[32m    448\u001b[39m \u001b[38;5;66;03m#print(vals, sent[:5])\u001b[39;00m\n\u001b[32m    449\u001b[39m \u001b[38;5;66;03m#except StopIteration:\u001b[39;00m\n\u001b[32m    450\u001b[39m \u001b[38;5;66;03m#    vals = ''\u001b[39;00m\n\u001b[32m    452\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(vals, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ochoa\\projects\\babel\\BabyLemmatizer\\model_api.py:43\u001b[39m, in \u001b[36mread_results\u001b[39m\u001b[34m(filename)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_results\u001b[39m(filename):\n\u001b[32m     42\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\" Read OpenNMT output file \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     44\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[32m     46\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m line.replace(\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m).rstrip()\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\ochoa\\\\AppData\\\\Local\\\\Temp\\\\babylem_tg_w0h1l\\\\temp.tag_pred'"
     ]
    }
   ],
   "source": [
    "ejemplo_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integración con pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ejemplo_pandas():\n",
    "    \"\"\"Exportar resultados a pandas DataFrame\"\"\"\n",
    "    print(\"\\n=== EJEMPLO 6: Integración con pandas ===\")\n",
    "    \n",
    "    import pandas as pd\n",
    "    \n",
    "    texto = [\n",
    "        \"e-nu-ma e-liš la na-bu-u₂ šamû\",\n",
    "        \"šap-liš am-ma-tum šu-ma la za-ak-rat\"\n",
    "    ]\n",
    "    \n",
    "    # Procesar\n",
    "    conllu = txt_lines_to_conllu(texto)\n",
    "    lem = Lemmatizer(conllu)\n",
    "    resultado = lem.run_model('lbtest2', cpu=True)\n",
    "    \n",
    "    # Convertir a DataFrame\n",
    "    data = []\n",
    "    for id_, form, lemma, xpos in resultado.get_contents('id', 'form', 'lemma', 'xpos'):\n",
    "        data.append({\n",
    "            'id': id_,\n",
    "            'form': form,\n",
    "            'lemma': lemma,\n",
    "            'pos': xpos\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    print(\"\\n✓ DataFrame creado:\")\n",
    "    print(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EJEMPLO 6: Integración con pandas ===\n",
      "> Your model was trained with an old version of BabyLemmatizer.\n",
      "> Using Tokenizer setting 0. Rebuild model using --tokenizer.\n",
      "> Using tokenizer 0\n",
      "> Your model was trained with an old version of BabyLemmatizer.\n",
      "> Using default contexts\n",
      "> Tagger context = 2\n",
      "> Lemmatizer context = 1\n",
      "> Normalizing CoNLL-U\n",
      "> Updating field \"formctx\"\n",
      "> Fetching contexts for \"form\"\n",
      "> Generating input data for neural net (memory mode)\n",
      "> Input file size: 10 words in 2 segments.\n",
      "> Tagging with lbtest2\n",
      "> Updating field \"xpos\"\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\ochoa\\\\AppData\\\\Local\\\\Temp\\\\babylem_wj6kjhkw\\\\temp.tag_pred'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mejemplo_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mejemplo_pandas\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     13\u001b[39m conllu = txt_lines_to_conllu(texto)\n\u001b[32m     14\u001b[39m lem = Lemmatizer(conllu)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m resultado = \u001b[43mlem\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_model\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlbtest2\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Convertir a DataFrame\u001b[39;00m\n\u001b[32m     18\u001b[39m data = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ochoa\\projects\\babel\\BabyLemmatizer\\lemmatizer_pipeline.py:193\u001b[39m, in \u001b[36mLemmatizer.run_model\u001b[39m\u001b[34m(self, model_name, cpu)\u001b[39m\n\u001b[32m    187\u001b[39m model_api.run_tagger(\u001b[38;5;28mself\u001b[39m.tagger_input,\n\u001b[32m    188\u001b[39m                      tagger_path,\n\u001b[32m    189\u001b[39m                      \u001b[38;5;28mself\u001b[39m.tagger_output,\n\u001b[32m    190\u001b[39m                      cpu)\n\u001b[32m    192\u001b[39m \u001b[38;5;66;03m# Merge tags to make lemmatizer input\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m \u001b[43mmodel_api\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmerge_tags\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtagger_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m                     \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msource_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m                     \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlemmatizer_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m                     \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mxpos\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m                     \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mxposctx\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[38;5;66;03m# Run lemmatizer\u001b[39;00m\n\u001b[32m    200\u001b[39m io(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mLemmatizing with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ochoa\\projects\\babel\\BabyLemmatizer\\model_api.py:69\u001b[39m, in \u001b[36mmerge_tags\u001b[39m\u001b[34m(neural_net_output, conllu_object, output_file, field, fieldctx)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m## TODO: tee tää suoraan tagger/lemmatisaattorikutsun\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m## yhteydessä, etenkin jos tästä tulee modulaarisempi\u001b[39;00m\n\u001b[32m     68\u001b[39m annotations = read_results(neural_net_output)\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m \u001b[43mconllu_object\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfield\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannotations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m contexts = {\u001b[33m'\u001b[39m\u001b[33mxpos\u001b[39m\u001b[33m'\u001b[39m: Context.lemmatizer_context,\n\u001b[32m     72\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mform\u001b[39m\u001b[33m'\u001b[39m: Context.tagger_context}\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fieldctx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ochoa\\projects\\babel\\BabyLemmatizer\\conlluplus.py:465\u001b[39m, in \u001b[36mConlluPlus.update_value\u001b[39m\u001b[34m(self, field, values)\u001b[39m\n\u001b[32m    457\u001b[39m     \u001b[38;5;66;03m#else:\u001b[39;00m\n\u001b[32m    458\u001b[39m     \u001b[38;5;66;03m#    vals = next(values)\u001b[39;00m\n\u001b[32m    459\u001b[39m     \u001b[38;5;66;03m#    for field, vals in zip(fields, zip(*vals)):\u001b[39;00m\n\u001b[32m    460\u001b[39m     \u001b[38;5;66;03m#        if isinstance(vals, (tuple, list)):\u001b[39;00m\n\u001b[32m    461\u001b[39m     \u001b[38;5;66;03m#            vals = '|'.join(vals)\u001b[39;00m\n\u001b[32m    462\u001b[39m     \u001b[38;5;66;03m#        sent[FIELDS[field]] = str(vals)\u001b[39;00m\n\u001b[32m    464\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m sent\n\u001b[32m--> \u001b[39m\u001b[32m465\u001b[39m \u001b[38;5;28mself\u001b[39m.data = \u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msents\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m             \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcomments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msents\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ochoa\\projects\\babel\\BabyLemmatizer\\conlluplus.py:465\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    457\u001b[39m     \u001b[38;5;66;03m#else:\u001b[39;00m\n\u001b[32m    458\u001b[39m     \u001b[38;5;66;03m#    vals = next(values)\u001b[39;00m\n\u001b[32m    459\u001b[39m     \u001b[38;5;66;03m#    for field, vals in zip(fields, zip(*vals)):\u001b[39;00m\n\u001b[32m    460\u001b[39m     \u001b[38;5;66;03m#        if isinstance(vals, (tuple, list)):\u001b[39;00m\n\u001b[32m    461\u001b[39m     \u001b[38;5;66;03m#            vals = '|'.join(vals)\u001b[39;00m\n\u001b[32m    462\u001b[39m     \u001b[38;5;66;03m#        sent[FIELDS[field]] = str(vals)\u001b[39;00m\n\u001b[32m    464\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m sent\n\u001b[32m--> \u001b[39m\u001b[32m465\u001b[39m \u001b[38;5;28mself\u001b[39m.data = [(comments, \u001b[43m[\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msents\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[32m    466\u001b[39m              \u001b[38;5;28;01mfor\u001b[39;00m comments, sents \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.data]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ochoa\\projects\\babel\\BabyLemmatizer\\conlluplus.py:465\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    457\u001b[39m     \u001b[38;5;66;03m#else:\u001b[39;00m\n\u001b[32m    458\u001b[39m     \u001b[38;5;66;03m#    vals = next(values)\u001b[39;00m\n\u001b[32m    459\u001b[39m     \u001b[38;5;66;03m#    for field, vals in zip(fields, zip(*vals)):\u001b[39;00m\n\u001b[32m    460\u001b[39m     \u001b[38;5;66;03m#        if isinstance(vals, (tuple, list)):\u001b[39;00m\n\u001b[32m    461\u001b[39m     \u001b[38;5;66;03m#            vals = '|'.join(vals)\u001b[39;00m\n\u001b[32m    462\u001b[39m     \u001b[38;5;66;03m#        sent[FIELDS[field]] = str(vals)\u001b[39;00m\n\u001b[32m    464\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m sent\n\u001b[32m--> \u001b[39m\u001b[32m465\u001b[39m \u001b[38;5;28mself\u001b[39m.data = [(comments, [\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sents])\n\u001b[32m    466\u001b[39m              \u001b[38;5;28;01mfor\u001b[39;00m comments, sents \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.data]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ochoa\\projects\\babel\\BabyLemmatizer\\conlluplus.py:447\u001b[39m, in \u001b[36mConlluPlus.update_value.<locals>.update\u001b[39m\u001b[34m(sent)\u001b[39m\n\u001b[32m    444\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m sent\n\u001b[32m    445\u001b[39m \u001b[38;5;66;03m#print(sent)\u001b[39;00m\n\u001b[32m    446\u001b[39m \u001b[38;5;66;03m#try:\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m447\u001b[39m vals = \u001b[38;5;28mnext\u001b[39m(values)\n\u001b[32m    448\u001b[39m \u001b[38;5;66;03m#print(vals, sent[:5])\u001b[39;00m\n\u001b[32m    449\u001b[39m \u001b[38;5;66;03m#except StopIteration:\u001b[39;00m\n\u001b[32m    450\u001b[39m \u001b[38;5;66;03m#    vals = ''\u001b[39;00m\n\u001b[32m    452\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(vals, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ochoa\\projects\\babel\\BabyLemmatizer\\model_api.py:43\u001b[39m, in \u001b[36mread_results\u001b[39m\u001b[34m(filename)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_results\u001b[39m(filename):\n\u001b[32m     42\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\" Read OpenNMT output file \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     44\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[32m     46\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m line.replace(\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m).rstrip()\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\ochoa\\\\AppData\\\\Local\\\\Temp\\\\babylem_wj6kjhkw\\\\temp.tag_pred'"
     ]
    }
   ],
   "source": [
    "ejemplo_pandas()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
